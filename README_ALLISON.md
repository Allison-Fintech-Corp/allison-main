# Allison Main – Local Dev Guide (AnythingLLM + OpenRouter)

This repository contains an AnythingLLM-based development environment tailored for Allison Fintech Corp. It is pre-configured to use OpenRouter as the global LLM provider, with sensible defaults for temperature and output length so contributors can get productive quickly.

If you’re new to the project, start here.

---

## Contents
- Overview
- Architecture and Ports
- Prerequisites
- First-Time Setup
- Environment Configuration
- Running the Stack (Development)
- Verifying the Setup (Smoke Tests)
- Developing with Workspaces
- LLM Behavior Controls
- Troubleshooting
- Security Notes
- Contribution Workflow

---

## Overview
- Base app: AnythingLLM (Mintplex Labs) – chat over documents, with vector DB and multiple LLM providers.
- Provider: OpenRouter only (global default set via env). Model can be changed via env or per workspace.
- Target: Enable a demo for CRA exam preparation workflows and later customization into an Allison-branded app.

## Architecture and Ports
- Frontend (Vite/React): http://localhost:3000
- Server (Express/API): http://localhost:3001/api
- Collector (Document processor): http://localhost:8888
- Vector DB: `lancedb` (local, embedded)
- Embeddings: `native` (local CPU embeddings)

## Prerequisites
- Node.js >= 18 (recommended LTS)
- Yarn 1.x
- GitHub access to this repository
- OpenRouter account + API Key

## First-Time Setup
1) Install dependencies and scaffold env files:

```bash
yarn setup
```

This installs packages for `server`, `frontend`, and `collector`, copies env examples, runs Prisma migrations and seeding.

2) Set OpenRouter envs in `server/.env.development` (not committed):

Required:
- `LLM_PROVIDER=openrouter`
- `OPENROUTER_API_KEY=sk-or-...` (keep secret)
- `OPENROUTER_MODEL_PREF=meta-llama/llama-3.3-70b-instruct` (or any supported OpenRouter model)

Recommended defaults (already wired in code):
- `LLM_DEFAULT_TEMPERATURE=0.2`  (global default; can be overridden per request/workspace)
- `OPENROUTER_MAX_TOKENS=1500`   (cap completion length)
- `OPENROUTER_TIMEOUT_MS=1200`   (optional, safeguards long-open streams)

Other important defaults:
- `VECTOR_DB=lancedb`
- `EMBEDDING_ENGINE=native`
- Strong secrets (autogenerated on first run): `JWT_SECRET`, `SIG_KEY`, `SIG_SALT`

3) Start development services in separate terminals or via one command:

- Separate tabs:
  - `yarn dev:server`
  - `yarn dev:frontend`
  - `yarn dev:collector`

- Or concurrently:
  - `yarn dev:all`

## Running the Stack (Development)
- Frontend: `yarn dev:frontend` (Vite on 3000)
- Server: `yarn dev:server` (Express on 3001)
- Collector: `yarn dev:collector` (Express on 8888)

If a port is taken, free it:
```bash
lsof -ti tcp:3000,3001,8888 | xargs -r kill -9
```
Then re-run the dev commands.

## Verifying the Setup (Smoke Tests)
- Open the UI: http://localhost:3000
- Complete initial setup (single-user by default)
- In Admin > LLM Settings: confirm provider is OpenRouter and default model matches `OPENROUTER_MODEL_PREF`.
- Create a workspace and send a simple message. You should get a concise, deterministic response (temp 0.2). Output length should cap around ~1200–1500 tokens.

API Check:
- Health: `GET http://localhost:3001/api/system/health`
- Basic OpenAI-compatible chat is routed internally and respects env defaults.

## Developing with Workspaces
- Import or drop documents; embeddings are created under the workspace slug using `lancedb`.
- Two chat modes:
  - Chat (general)
  - Query (retrieval over documents)
- In Query mode, if no matching context is found, the server returns a refusal response instead of hallucinating.

## LLM Behavior Controls
Global via env:
- `OPENROUTER_MODEL_PREF` – default model slug
- `LLM_DEFAULT_TEMPERATURE` – default temperature (0–2; we clamp to this range)
- `OPENROUTER_MAX_TOKENS` – caps completion tokens sent to OpenRouter
- `OPENROUTER_TIMEOUT_MS` – stream idle cutoff; prevents hanging responses

Per request/workspace:
- The chat layer uses the following precedence for temperature:
  - Explicit per-request temperature
  - Workspace `openAiTemp`
  - Provider default (`LLM_DEFAULT_TEMPERATURE`)

Code references:
- OpenRouter provider: `server/utils/AiProviders/openRouter/index.js`
- Chat controller: `server/utils/chats/openaiCompatible.js`
- LLM provider selection: `server/utils/helpers/index.js#getLLMProvider()`

## Troubleshooting
- Port conflicts: free ports with `lsof -ti tcp:3000,3001,8888 | xargs -r kill -9`
- OpenRouter key errors: ensure `OPENROUTER_API_KEY` is present in `server/.env.development`
- Model invalid: verify `OPENROUTER_MODEL_PREF` is a valid OpenRouter model id
- Streaming hangs: increase `OPENROUTER_TIMEOUT_MS` (e.g., 1500–2500)
- Embeddings slow: first run downloads can be slow; subsequent runs are cached
- Vector search empty in Query mode: add documents and re-try; app will refuse to answer with no context

## Security Notes
- Never commit `.env` files or secrets. They’re gitignored (`server/.env.development`, `frontend/.env`, `collector/.env`).
- Rotate OpenRouter API keys if they are ever exposed.
- PRs should avoid printing secrets in logs; sanitize debug output.

## Contribution Workflow
- Default branch: `main`
- Small changes to production code can be pushed to `main`, but for larger features use branches and PRs.
- Commit style: concise subject + brief body when useful.
- Before pushing:
  - Ensure `yarn lint` passes (server, frontend, collector)
  - Basic local smoke tests pass

If you need help or run into issues, open an issue with steps to reproduce and logs from server, frontend, and collector.
